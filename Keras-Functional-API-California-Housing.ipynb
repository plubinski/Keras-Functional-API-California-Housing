{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0-preview is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "np.random.seed(1981)\n",
    "tf.random.set_seed(1981)\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.0-alpha0', '2.2.4-tf')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API\n",
    "\n",
    "### DATASET\n",
    "This dataset is a modified version of the California Housing dataset available from Luís Torgo's page (University of Porto). Luís Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.\n",
    "\n",
    "The following is the description from the book author:\n",
    "\n",
    "This dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "The dataset in this directory is almost identical to the original, with two differences: 207 values were randomly removed from the total_bedrooms column, so we can discuss what to do with missing data. An additional categorical attribute called ocean_proximity was added, indicating (very roughly) whether each block group is near the ocean, near the Bay area, inland or on an island. This allows discussing what to do with categorical data. Note that the block groups are called \"districts\" in the Jupyter notebooks, simply because in some contexts the name \"block group\" was confusing.\"\n",
    "\n",
    "About the Data (From Luís Torgo page):\n",
    "http://www.dcc.fc.up.pt/%7Eltorgo/Regression/cal_housing.html\n",
    "\n",
    "This is a dataset obtained from the StatLib repository. Here is the included description:\n",
    "\n",
    "\"We collected information on the variables using all the block groups in California from the 1990 Cens us. In this sample a block group on average includes 1425.5 individuals living in a geographically co mpact area. Naturally, the geographical area included varies inversely with the population density. W e computed distances among the centroids of each block group as measured in latitude and longitude. W e excluded all the block groups reporting zero entries for the independent and dependent variables. T he final data contained 20,640 observations on 9 variables. The dependent variable is ln(median house value).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 1.7935 - val_loss: 1.1242\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.7584 - val_loss: 0.7740\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.6860 - val_loss: 0.6590\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.6365 - val_loss: 0.5946\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5967 - val_loss: 0.5593\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.5621 - val_loss: 0.5319\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.5330 - val_loss: 0.5065\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5091 - val_loss: 0.4926\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4894 - val_loss: 0.4760\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4732 - val_loss: 0.4638\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4596 - val_loss: 0.4558\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4490 - val_loss: 0.4639\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4401 - val_loss: 0.4424\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4328 - val_loss: 0.4541\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4271 - val_loss: 0.4307\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4221 - val_loss: 0.4477\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4178 - val_loss: 0.4663\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4139 - val_loss: 0.4364\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4113 - val_loss: 0.4242\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4084 - val_loss: 0.4190\n",
      "5160/5160 [==============================] - 0s 16us/sample - loss: 0.3952\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.666851 ],\n",
       "       [1.5663123],\n",
       "       [3.6176703]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API\n",
    "Not all neural network models are simply sequential. Some may have complex topologies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 30)           270         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 30)           930         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 38)           0           input_4[0][0]                    \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            39          concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 2.1116 - val_loss: 2.5496\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.7438 - val_loss: 0.7749\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.6542 - val_loss: 0.6222\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.6047 - val_loss: 0.5735\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.5674 - val_loss: 0.5438\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.5394 - val_loss: 0.5152\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.5162 - val_loss: 0.4896\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4979 - val_loss: 0.4774\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4831 - val_loss: 0.4574\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4703 - val_loss: 0.4465\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4602 - val_loss: 0.4348\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4517 - val_loss: 0.4347\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4443 - val_loss: 0.4281\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4380 - val_loss: 0.4214\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4325 - val_loss: 0.4126\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4279 - val_loss: 0.4116\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4242 - val_loss: 0.4022\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4201 - val_loss: 0.4006\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4166 - val_loss: 0.3981\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4132 - val_loss: 0.3954\n",
      "5160/5160 [==============================] - 0s 16us/sample - loss: 0.4168\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5])\n",
    "input_B = keras.layers.Input(shape=[6])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 2.1379 - val_loss: 3.2553\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.8744 - val_loss: 1.2832\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.7094 - val_loss: 0.8046\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.6440 - val_loss: 0.6550\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.6031 - val_loss: 0.5875\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.5723 - val_loss: 0.5509\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.5471 - val_loss: 0.5247\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.5261 - val_loss: 0.5028\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5086 - val_loss: 0.4908\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4942 - val_loss: 0.4784\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4827 - val_loss: 0.4702\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4727 - val_loss: 0.4661\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4639 - val_loss: 0.4658\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4566 - val_loss: 0.4681\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4503 - val_loss: 0.4687\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4452 - val_loss: 0.4777\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4411 - val_loss: 0.4774\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4374 - val_loss: 0.4755\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4343 - val_loss: 0.4764\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4312 - val_loss: 0.4797\n",
      "5160/5160 [==============================] - 0s 18us/sample - loss: 0.4194\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an auxiliary output for regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5])\n",
    "input_B = keras.layers.Input(shape=[6])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "aux_output = keras.layers.Dense(1)(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4748 - dense_25_loss: 0.4286 - dense_26_loss: 0.8910 - val_loss: 0.4516 - val_dense_25_loss: 0.4007 - val_dense_26_loss: 0.9096\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4715 - dense_25_loss: 0.4265 - dense_26_loss: 0.8757 - val_loss: 0.4468 - val_dense_25_loss: 0.3973 - val_dense_26_loss: 0.8926\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4685 - dense_25_loss: 0.4249 - dense_26_loss: 0.8613 - val_loss: 0.4512 - val_dense_25_loss: 0.4038 - val_dense_26_loss: 0.8780\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4659 - dense_25_loss: 0.4234 - dense_26_loss: 0.8486 - val_loss: 0.4407 - val_dense_25_loss: 0.3937 - val_dense_26_loss: 0.8637\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4634 - dense_25_loss: 0.4219 - dense_26_loss: 0.8363 - val_loss: 0.4407 - val_dense_25_loss: 0.3951 - val_dense_26_loss: 0.8515\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4611 - dense_25_loss: 0.4207 - dense_26_loss: 0.8245 - val_loss: 0.4365 - val_dense_25_loss: 0.3918 - val_dense_26_loss: 0.8386\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4587 - dense_25_loss: 0.4193 - dense_26_loss: 0.8130 - val_loss: 0.4449 - val_dense_25_loss: 0.4020 - val_dense_26_loss: 0.8309\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4565 - dense_25_loss: 0.4181 - dense_26_loss: 0.8024 - val_loss: 0.4327 - val_dense_25_loss: 0.3902 - val_dense_26_loss: 0.8153\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4544 - dense_25_loss: 0.4168 - dense_26_loss: 0.7922 - val_loss: 0.4332 - val_dense_25_loss: 0.3916 - val_dense_26_loss: 0.8083\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4521 - dense_25_loss: 0.4154 - dense_26_loss: 0.7820 - val_loss: 0.4281 - val_dense_25_loss: 0.3875 - val_dense_26_loss: 0.7942\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4502 - dense_25_loss: 0.4143 - dense_26_loss: 0.7729 - val_loss: 0.4290 - val_dense_25_loss: 0.3890 - val_dense_26_loss: 0.7897\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4480 - dense_25_loss: 0.4130 - dense_26_loss: 0.7631 - val_loss: 0.4243 - val_dense_25_loss: 0.3849 - val_dense_26_loss: 0.7785\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4463 - dense_25_loss: 0.4121 - dense_26_loss: 0.7543 - val_loss: 0.4282 - val_dense_25_loss: 0.3908 - val_dense_26_loss: 0.7649\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4445 - dense_25_loss: 0.4109 - dense_26_loss: 0.7462 - val_loss: 0.4221 - val_dense_25_loss: 0.3848 - val_dense_26_loss: 0.7572\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4427 - dense_25_loss: 0.4099 - dense_26_loss: 0.7376 - val_loss: 0.4320 - val_dense_25_loss: 0.3955 - val_dense_26_loss: 0.7610\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4410 - dense_25_loss: 0.4090 - dense_26_loss: 0.7292 - val_loss: 0.4181 - val_dense_25_loss: 0.3818 - val_dense_26_loss: 0.7445\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4395 - dense_25_loss: 0.4080 - dense_26_loss: 0.7225 - val_loss: 0.4177 - val_dense_25_loss: 0.3818 - val_dense_26_loss: 0.7408\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4379 - dense_25_loss: 0.4071 - dense_26_loss: 0.7154 - val_loss: 0.4161 - val_dense_25_loss: 0.3808 - val_dense_26_loss: 0.7336\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4363 - dense_25_loss: 0.4061 - dense_26_loss: 0.7082 - val_loss: 0.4139 - val_dense_25_loss: 0.3790 - val_dense_26_loss: 0.7278\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4349 - dense_25_loss: 0.4052 - dense_26_loss: 0.7022 - val_loss: 0.4221 - val_dense_25_loss: 0.3878 - val_dense_26_loss: 0.7306\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 24us/sample - loss: 0.4728 - dense_25_loss: 0.4271 - dense_26_loss: 0.8844\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
